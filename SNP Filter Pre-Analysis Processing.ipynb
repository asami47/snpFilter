{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654e27ab-32f2-480e-b98f-8d86a02e9d19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import regexp_replace, col,trim, dense_rank, percent_rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "from functools import reduce\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3958dd-e7e9-4c16-9580-cbcb90943325",
     "showTitle": true,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path='dbfs:/FileStore/tables/dna_normal'\n",
    "dbutils.fs.ls(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f1a5fc-e003-4567-a825-efe668a002f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.mkdirs('dbfs:/FileStore/tables/dna_variants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d0f0fbe-7bd5-4076-8c81-1945f35a9373",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.mkdirs('dbfs:/FileStore/tables/dna_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0650afb-8194-4a8c-9355-f401a8b6b97e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170e0b59-ec72-4274-8ccc-ebec6008b612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "f1 = \"/FileStore/tables/dna_normal/sample__1_.txt\"\n",
    "f2 = \"/FileStore/tables/dna_normal/sample__2_.txt\"\n",
    "f3 = \"/FileStore/tables/dna_normal/sample__3_.txt\"\n",
    "f4 = \"/FileStore/tables/dna_normal/sample__4_.txt\"\n",
    "f5 = \"/FileStore/tables/dna_normal/sample__5_.txt\"\n",
    "f6 = \"/FileStore/tables/dna_normal/sample__6_.txt\"\n",
    "f7 = \"/FileStore/tables/dna_normal/sample__7_.txt\"\n",
    "f8 = \"/FileStore/tables/dna_normal/sample__8_.txt\"\n",
    "f9 = \"/FileStore/tables/dna_normal/sample__9_.txt\"\n",
    "f10 = \"/FileStore/tables/dna_normal/sample__10_.txt\"\n",
    "f11 = \"/FileStore/tables/dna_normal/sample__11_.txt\"\n",
    "f12 = \"/FileStore/tables/dna_normal/sample__12_.txt\"\n",
    "f13 = \"/FileStore/tables/dna_normal/sample__13_.txt\"\n",
    "f14 = \"/FileStore/tables/dna_normal/sample__14_.txt\"\n",
    "f15 = \"/FileStore/tables/dna_normal/sample__15_.txt\"\n",
    "f16 = \"/FileStore/tables/dna_normal/sample__16_.txt\"\n",
    "f17 = \"/FileStore/tables/dna_normal/sample__17_.txt\"\n",
    "f18 = \"/FileStore/tables/dna_normal/sample__18_.txt\"\n",
    "f19 = \"/FileStore/tables/dna_normal/sample__19_.txt\"\n",
    "f20 = \"/FileStore/tables/dna_normal/sample__20_.txt\"\n",
    "f21 = \"/FileStore/tables/dna_normal/sample__21_.txt\"\n",
    "f22 = \"/FileStore/tables/dna_normal/sample__22_.txt\"\n",
    "f23 = \"/FileStore/tables/dna_normal/sample__23_.txt\"\n",
    "f24 = \"/FileStore/tables/dna_normal/sample__24_.txt\"\n",
    "f25 = \"/FileStore/tables/dna_normal/sample__25_.txt\"\n",
    "f26 = \"/FileStore/tables/dna_normal/sample__26_.txt\"\n",
    "f27 = \"/FileStore/tables/dna_normal/sample__27_.txt\"\n",
    "f28 = \"/FileStore/tables/dna_normal/sample__28_.txt\"\n",
    "f29 = \"/FileStore/tables/dna_normal/sample__29_.txt\"\n",
    "f30 = \"/FileStore/tables/dna_normal/sample__30_.txt\"\n",
    "f31 = \"/FileStore/tables/dna_normal/sample__31_.txt\"\n",
    "f32 = \"/FileStore/tables/dna_normal/sample__32_.txt\"\n",
    "f33 = \"/FileStore/tables/dna_normal/sample__33_.txt\"\n",
    "f34 = \"/FileStore/tables/dna_normal/sample__34_.txt\"\n",
    "f35 = \"/FileStore/tables/dna_normal/sample__35_.txt\"\n",
    "f36 = \"/FileStore/tables/dna_normal/sample__36_.txt\"\n",
    "f37 = \"/FileStore/tables/dna_normal/sample__37_.txt\"\n",
    "f38 = \"/FileStore/tables/dna_normal/sample__38_.txt\"\n",
    "f39 = \"/FileStore/tables/dna_normal/sample__39_.txt\"\n",
    "f40 = \"/FileStore/tables/dna_normal/sample__40_.txt\"\n",
    "f41 = \"/FileStore/tables/dna_normal/sample__41_.txt\"\n",
    "f42 = \"/FileStore/tables/dna_normal/sample__42_.txt\"\n",
    "f43 = \"/FileStore/tables/dna_normal/sample__43_.txt\"\n",
    "f44 = \"/FileStore/tables/dna_normal/sample__44_.txt\"\n",
    "f45 = \"/FileStore/tables/dna_normal/sample__45_.txt\"\n",
    "f46 = \"/FileStore/tables/dna_normal/sample__46_.txt\"\n",
    "f47 = \"/FileStore/tables/dna_normal/sample__47_.txt\"\n",
    "f48 = \"/FileStore/tables/dna_normal/sample__48_.txt\"\n",
    "f49 = \"/FileStore/tables/dna_normal/sample__49_.txt\"\n",
    "f50 = \"/FileStore/tables/dna_normal/sample__50_.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df1= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f1).withColumn('filename',lit('f1'))\n",
    "df2= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f2).withColumn('filename',lit('f2'))\n",
    "df3= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f3).withColumn('filename',lit('f3'))\n",
    "df4= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f4).withColumn('filename',lit('f4'))\n",
    "df5= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f5).withColumn('filename',lit('f5'))\n",
    "df6= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f6).withColumn('filename',lit('f6'))\n",
    "df7= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f7).withColumn('filename',lit('f7'))\n",
    "df8= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f8).withColumn('filename',lit('f8'))\n",
    "df9= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f9).withColumn('filename',lit('f9'))\n",
    "df10= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f10).withColumn('filename',lit('f10'))\n",
    "df11= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f11).withColumn('filename',lit('f11'))\n",
    "df12= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f12).withColumn('filename',lit('f12'))\n",
    "df13= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f13).withColumn('filename',lit('f13'))\n",
    "df14= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f14).withColumn('filename',lit('f14'))\n",
    "df15= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f15).withColumn('filename',lit('f15'))\n",
    "df16= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f16).withColumn('filename',lit('f16'))\n",
    "df17= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f17).withColumn('filename',lit('f17'))\n",
    "df18= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f18).withColumn('filename',lit('f18'))\n",
    "df19= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f19).withColumn('filename',lit('f19'))\n",
    "df20= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f20).withColumn('filename',lit('f20'))\n",
    "df21= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f21).withColumn('filename',lit('f21'))\n",
    "df22= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f22).withColumn('filename',lit('f22'))\n",
    "df23= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f23).withColumn('filename',lit('f23'))\n",
    "df24= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f24).withColumn('filename',lit('f24'))\n",
    "df25= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f25).withColumn('filename',lit('f25'))\n",
    "df26= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f26).withColumn('filename',lit('f26'))\n",
    "df27= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f27).withColumn('filename',lit('f27'))\n",
    "df28= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f28).withColumn('filename',lit('f28'))\n",
    "df29= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f29).withColumn('filename',lit('f29'))\n",
    "df30= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f30).withColumn('filename',lit('f30'))\n",
    "df31= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f31).withColumn('filename',lit('f31'))\n",
    "df32= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f32).withColumn('filename',lit('f32'))\n",
    "df33= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f33).withColumn('filename',lit('f33'))\n",
    "df34= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f34).withColumn('filename',lit('f34'))\n",
    "df35= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f35).withColumn('filename',lit('f35'))\n",
    "df36= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f36).withColumn('filename',lit('f36'))\n",
    "df37= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f37).withColumn('filename',lit('f37'))\n",
    "df38= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f38).withColumn('filename',lit('f38'))\n",
    "df39= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f39).withColumn('filename',lit('f39'))\n",
    "df40= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f40).withColumn('filename',lit('f40'))\n",
    "df41= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f41).withColumn('filename',lit('f41'))\n",
    "df42= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f42).withColumn('filename',lit('f42'))\n",
    "df43= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f43).withColumn('filename',lit('f43'))\n",
    "df44= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f44).withColumn('filename',lit('f44'))\n",
    "df45= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f45).withColumn('filename',lit('f45'))\n",
    "df46= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f46).withColumn('filename',lit('f46'))\n",
    "df47= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f47).withColumn('filename',lit('f47'))\n",
    "df48= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f48).withColumn('filename',lit('f48'))\n",
    "df49= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f49).withColumn('filename',lit('f49'))\n",
    "df50= spark.read.format('csv').option(\"inferSchema\", True).option(\"header\", True).option(\"sep\", '\\t').load(f50).withColumn('filename',lit('f50'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e69f62-f69d-431e-9889-f31005497fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# create a list to hold the dataframes\n",
    "dfs = []\n",
    "\n",
    "# loop over the dataframe names and add them to the list\n",
    "for i in range(1, 51):\n",
    "    df_name = f\"df{i}\"\n",
    "    if df_name in locals() and isinstance(locals()[df_name], DataFrame):\n",
    "        dfs.append(locals()[df_name])\n",
    "\n",
    "# concatenate the dataframes\n",
    "df = reduce(DataFrame.union, dfs)\n",
    "df6.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9dfca856-cf45-4011-9f50-dd3474ec8fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ccf10e-cd97-4b48-ae37-8ae583d7f021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_path = \"/FileStore/tables/processed/processed_table\"\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb34c5a6-99a9-41b7-ba2d-7e8e8d2a5aa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SNP Filter Pre-Analysis Processing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "snpFilter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
